import pandas as pd
import cv2
import os
import matplotlib.pyplot as plt
import numpy as np

from PIL import Image

import tensorflow as tf
tf.debugging.enable_check_numerics()
from keras.applications.vgg16 import VGG16
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator

data_path = 'C:/Users/Jang Dong Min/Desktop/dataset'

# ImageDataGenerator 생성
datagen = ImageDataGenerator(
    rescale=1./255,      # 픽셀값 범위를 0~1로 변환
    rotation_range=20,   # 이미지 회전 각도 범위
    zoom_range=0.2,      # 이미지 확대/축소 범위
    width_shift_range=0.2, # 이미지 가로 이동 범위
    height_shift_range=0.2, # 이미지 세로 이동 범위
    horizontal_flip=True,
    vertical_flip=True
)

# 데이터 경로 정의
train_data_path = os.path.join(data_path, 'train')
val_data_path = os.path.join(data_path, 'val')
test_data_path = os.path.join(data_path, 'test')

# 데이터 생성기 정의

# 훈련 데이터 불러오기
train_generator = datagen.flow_from_directory(
    train_data_path,
    target_size=(32, 32),
    batch_size=64,
    class_mode='categorical'
)

# 검증 데이터 생성
val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(
    val_data_path,
    target_size=(32, 32),
    batch_size=64,
    class_mode='categorical'
)


# 테스트 데이터 불러오기
test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(
    test_data_path,
    target_size=(32, 32),
    batch_size=64,
    class_mode='categorical'
)


# VGG16 모델 불러오기
vgg_model = VGG16(include_top=False, input_shape=(32, 32, 3))

# VGG16 모델의 출력을 추가적인 레이어로 연결하기
model = Sequential()
model.add(vgg_model)
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(14, activation='softmax'))

# VGG16 모델의 레이어는 훈련되지 않도록 고정
for layer in vgg_model.layers:
    layer.trainable = False

# 모델 컴파일
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# ModelCheckpoint Callback 함수
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max')

# 학습 데이터에서도 모델의 성능을 모니터링하도록 추가
checkpoint_train = ModelCheckpoint('best_model_train.h5', monitor='accuracy', save_best_only=True, mode='max')

# EarlyStopping Callback 함수
early_stop = EarlyStopping(
    monitor='val_accuracy',
    min_delta=0.001,  # 최소한의 개선을 감지할 수 있도록 함
    patience=5,  # 5 epoch 이상 개선이 없으면 학습 중단
    mode='max',  # 최대값을 갖는 성능 지표를 사용
    baseline=None,  # 최초의 성능 지표를 기준으로 함
    restore_best_weights=True,  # 최적의 가중치를 복원함
    verbose=1  # 상세한 정보 출력
)

# ReduceLROnPlateau Callback 함수
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss', 
    factor=0.2, 
    patience=3,
    min_lr=1e-6,  # 학습률이 1e-6보다 작아지지 않도록 함
    cooldown=2,   # 학습률 감소 이벤트 후 2 epoch 동안 다른 콜백 함수가 실행되지 않도록 함
    verbose=1
)

# 정확도와 손실 그래프 출력 함수
def plot_history(history):
    fig, ax = plt.subplots(2, 1, figsize=(10, 10))

    ax[0].plot(history.history['accuracy'])
    ax[0].plot(history.history['val_accuracy'])
    ax[0].set_title('Model Accuracy')
    ax[0].set_ylabel('Accuracy')
    ax[0].set_xlabel('Epoch')
    ax[0].legend(['Train', 'Validation'], loc='upper left')

    ax[1].plot(history.history['loss'])
    ax[1].plot(history.history['val_loss'])
    ax[1].set_title('Model Loss')
    ax[1].set_ylabel('Loss')
    ax[1].set_xlabel('Epoch')
    ax[1].legend(['Train', 'Validation'], loc='upper left')

    plt.show()
    
model.summary()
    
# 모델 학습
history = model.fit(train_generator, epochs=20, batch_size=64, 
                    validation_data=val_generator, validation_steps=len(val_generator), 
                    callbacks=[checkpoint, checkpoint_train, early_stop, reduce_lr])

# 모델 평가
score = model.evaluate(test_generator, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

# 학습 과정 그래프 출력
plot_history(history)

_________________________________________________________________
Epoch 1/20
668/668 [==============================] - 468s 697ms/step - loss: 1.0844 - accuracy: 0.6381 - val_loss: 0.5679 - val_accuracy: 0.8306 - lr: 0.0010
Epoch 2/20
668/668 [==============================] - 446s 667ms/step - loss: 0.7764 - accuracy: 0.7364 - val_loss: 0.5077 - val_accuracy: 0.8342 - lr: 0.0010
Epoch 3/20
668/668 [==============================] - 460s 688ms/step - loss: 0.7086 - accuracy: 0.7581 - val_loss: 0.4701 - val_accuracy: 0.8413 - lr: 0.0010
Epoch 4/20
668/668 [==============================] - 463s 693ms/step - loss: 0.6705 - accuracy: 0.7726 - val_loss: 0.4198 - val_accuracy: 0.8617 - lr: 0.0010
Epoch 5/20
668/668 [==============================] - 457s 684ms/step - loss: 0.6504 - accuracy: 0.7773 - val_loss: 0.4081 - val_accuracy: 0.8630 - lr: 0.0010
Epoch 6/20
668/668 [==============================] - 464s 695ms/step - loss: 0.6219 - accuracy: 0.7851 - val_loss: 0.3756 - val_accuracy: 0.8752 - lr: 0.0010
Epoch 7/20
668/668 [==============================] - 458s 686ms/step - loss: 0.6158 - accuracy: 0.7912 - val_loss: 0.3767 - val_accuracy: 0.8741 - lr: 0.0010
Epoch 8/20
668/668 [==============================] - 452s 677ms/step - loss: 0.5968 - accuracy: 0.7954 - val_loss: 0.3674 - val_accuracy: 0.8752 - lr: 0.0010
Epoch 9/20
668/668 [==============================] - 451s 675ms/step - loss: 0.5850 - accuracy: 0.7989 - val_loss: 0.3453 - val_accuracy: 0.8848 - lr: 0.0010
Epoch 10/20
668/668 [==============================] - 449s 671ms/step - loss: 0.5877 - accuracy: 0.7984 - val_loss: 0.3567 - val_accuracy: 0.8784 - lr: 0.0010
Epoch 11/20
668/668 [==============================] - 447s 669ms/step - loss: 0.5764 - accuracy: 0.7998 - val_loss: 0.3598 - val_accuracy: 0.8767 - lr: 0.0010
Epoch 12/20
668/668 [==============================] - 446s 668ms/step - loss: 0.5707 - accuracy: 0.8035 - val_loss: 0.3319 - val_accuracy: 0.8906 - lr: 0.0010
Epoch 13/20
668/668 [==============================] - 452s 677ms/step - loss: 0.5604 - accuracy: 0.8078 - val_loss: 0.3545 - val_accuracy: 0.8790 - lr: 0.0010
Epoch 14/20
668/668 [==============================] - 451s 675ms/step - loss: 0.5601 - accuracy: 0.8063 - val_loss: 0.3163 - val_accuracy: 0.8936 - lr: 0.0010
Epoch 15/20
668/668 [==============================] - 451s 675ms/step - loss: 0.5562 - accuracy: 0.8097 - val_loss: 0.3189 - val_accuracy: 0.8898 - lr: 0.0010
Epoch 16/20
668/668 [==============================] - 460s 688ms/step - loss: 0.5532 - accuracy: 0.8109 - val_loss: 0.3141 - val_accuracy: 0.8921 - lr: 0.0010
Epoch 17/20
668/668 [==============================] - 452s 676ms/step - loss: 0.5487 - accuracy: 0.8118 - val_loss: 0.3398 - val_accuracy: 0.8803 - lr: 0.0010
Epoch 18/20
668/668 [==============================] - 454s 679ms/step - loss: 0.5440 - accuracy: 0.8163 - val_loss: 0.3062 - val_accuracy: 0.8934 - lr: 0.0010
Epoch 19/20
668/668 [==============================] - 446s 667ms/step - loss: 0.5422 - accuracy: 0.8119 - val_loss: 0.3120 - val_accuracy: 0.8960 - lr: 0.0010
Epoch 20/20
668/668 [==============================] - 451s 675ms/step - loss: 0.5319 - accuracy: 0.8198 - val_loss: 0.3173 - val_accuracy: 0.8958 - lr: 0.0010
Test loss: 0.3408411741256714
Test accuracy: 0.8818147778511047
