import pandas as pd
import cv2
import os
import matplotlib.pyplot as plt
import numpy as np

from PIL import Image

import tensorflow as tf
tf.debugging.enable_check_numerics()
from keras.applications.vgg16 import VGG16
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator

data_path = 'C:/Users/Jang Dong Min/Desktop/dataset'

# ImageDataGenerator 생성
datagen = ImageDataGenerator(
    rescale=1./255,      # 픽셀값 범위를 0~1로 변환
    rotation_range=20,   # 이미지 회전 각도 범위
    zoom_range=0.2,      # 이미지 확대/축소 범위
    width_shift_range=0.2, # 이미지 가로 이동 범위
    height_shift_range=0.2, # 이미지 세로 이동 범위
    horizontal_flip=True,
    vertical_flip=True
)

# 데이터 경로 정의
train_data_path = os.path.join(data_path, 'train')
val_data_path = os.path.join(data_path, 'val')
test_data_path = os.path.join(data_path, 'test')

# 데이터 생성기 정의

# 훈련 데이터 불러오기
train_generator = datagen.flow_from_directory(
    train_data_path,
    target_size=(32, 32),
    batch_size=64,
    class_mode='categorical'
)

# 검증 데이터 생성
val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(
    val_data_path,
    target_size=(32, 32),
    batch_size=64,
    class_mode='categorical'
)


# 테스트 데이터 불러오기
test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(
    test_data_path,
    target_size=(32, 32),
    batch_size=64,
    class_mode='categorical'
)


# VGG16 모델 불러오기
vgg_model = VGG16(include_top=False, input_shape=(32, 32, 3))

# VGG16 모델의 출력을 추가적인 레이어로 연결하기
model = Sequential()
model.add(vgg_model)
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(14, activation='softmax'))

# VGG16 모델의 레이어는 훈련되지 않도록 고정
for layer in vgg_model.layers:
    layer.trainable = False

# 모델 컴파일
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# ModelCheckpoint Callback 함수
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)

# EarlyStopping Callback 함수
early_stop = EarlyStopping(monitor='val_accuracy', patience=5)

# ReduceLROnPlateau Callback 함수
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3)

# 정확도와 손실 그래프 출력 함수
def plot_history(history):
    fig, ax = plt.subplots(2, 1, figsize=(10, 10))

    ax[0].plot(history.history['accuracy'])
    ax[0].plot(history.history['val_accuracy'])
    ax[0].set_title('Model Accuracy')
    ax[0].set_ylabel('Accuracy')
    ax[0].set_xlabel('Epoch')
    ax[0].legend(['Train', 'Validation'], loc='upper left')

    ax[1].plot(history.history['loss'])
    ax[1].plot(history.history['val_loss'])
    ax[1].set_title('Model Loss')
    ax[1].set_ylabel('Loss')
    ax[1].set_xlabel('Epoch')
    ax[1].legend(['Train', 'Validation'], loc='upper left')

    plt.show()
    
model.summary()
    
# 모델 학습
history = model.fit(train_generator, epochs=20, batch_size=64, validation_data=val_generator,
                    validation_steps=len(val_generator), callbacks=[checkpoint, early_stop, reduce_lr])

# 모델 평가
score = model.evaluate(test_generator, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

# 학습 과정 그래프 출력
plot_history(history)


_________________________________________________________________
Epoch 1/20
668/668 [==============================] - 507s 754ms/step - loss: 1.1130 - accuracy: 0.6264 - val_loss: 0.5911 - val_accuracy: 0.8211 - lr: 0.0010
Epoch 2/20
668/668 [==============================] - 501s 750ms/step - loss: 0.7860 - accuracy: 0.7323 - val_loss: 0.4974 - val_accuracy: 0.8385 - lr: 0.0010
Epoch 3/20
668/668 [==============================] - 501s 749ms/step - loss: 0.7241 - accuracy: 0.7539 - val_loss: 0.4528 - val_accuracy: 0.8499 - lr: 0.0010
Epoch 4/20
668/668 [==============================] - 501s 750ms/step - loss: 0.6810 - accuracy: 0.7653 - val_loss: 0.4201 - val_accuracy: 0.8597 - lr: 0.0010
Epoch 5/20
668/668 [==============================] - 482s 722ms/step - loss: 0.6559 - accuracy: 0.7752 - val_loss: 0.4237 - val_accuracy: 0.8589 - lr: 0.0010
Epoch 6/20
668/668 [==============================] - 454s 680ms/step - loss: 0.6449 - accuracy: 0.7819 - val_loss: 0.3921 - val_accuracy: 0.8688 - lr: 0.0010
Epoch 7/20
668/668 [==============================] - 466s 697ms/step - loss: 0.6246 - accuracy: 0.7875 - val_loss: 0.3644 - val_accuracy: 0.8805 - lr: 0.0010
Epoch 8/20
668/668 [==============================] - 474s 710ms/step - loss: 0.6139 - accuracy: 0.7897 - val_loss: 0.3667 - val_accuracy: 0.8746 - lr: 0.0010
Epoch 9/20
668/668 [==============================] - 462s 692ms/step - loss: 0.6078 - accuracy: 0.7922 - val_loss: 0.3514 - val_accuracy: 0.8816 - lr: 0.0010
Epoch 10/20
668/668 [==============================] - 451s 674ms/step - loss: 0.5894 - accuracy: 0.7955 - val_loss: 0.3717 - val_accuracy: 0.8718 - lr: 0.0010
Epoch 11/20
668/668 [==============================] - 460s 689ms/step - loss: 0.5894 - accuracy: 0.7987 - val_loss: 0.3584 - val_accuracy: 0.8784 - lr: 0.0010
Epoch 12/20
668/668 [==============================] - 465s 696ms/step - loss: 0.5867 - accuracy: 0.7987 - val_loss: 0.3572 - val_accuracy: 0.8767 - lr: 0.0010
Epoch 13/20
668/668 [==============================] - 469s 702ms/step - loss: 0.5422 - accuracy: 0.8150 - val_loss: 0.3313 - val_accuracy: 0.8893 - lr: 2.0000e-04
Epoch 14/20
668/668 [==============================] - 473s 708ms/step - loss: 0.5314 - accuracy: 0.8203 - val_loss: 0.3270 - val_accuracy: 0.8921 - lr: 2.0000e-04
Epoch 15/20
668/668 [==============================] - 460s 689ms/step - loss: 0.5280 - accuracy: 0.8193 - val_loss: 0.3240 - val_accuracy: 0.8900 - lr: 2.0000e-04
Epoch 16/20
668/668 [==============================] - 454s 680ms/step - loss: 0.5338 - accuracy: 0.8181 - val_loss: 0.3190 - val_accuracy: 0.8923 - lr: 2.0000e-04
Epoch 17/20
668/668 [==============================] - 452s 677ms/step - loss: 0.5296 - accuracy: 0.8201 - val_loss: 0.3182 - val_accuracy: 0.8915 - lr: 2.0000e-04
Epoch 18/20
668/668 [==============================] - 453s 678ms/step - loss: 0.5273 - accuracy: 0.8207 - val_loss: 0.3231 - val_accuracy: 0.8896 - lr: 2.0000e-04
Epoch 19/20
668/668 [==============================] - 453s 677ms/step - loss: 0.5220 - accuracy: 0.8242 - val_loss: 0.3175 - val_accuracy: 0.8932 - lr: 2.0000e-04
Epoch 20/20
668/668 [==============================] - 454s 679ms/step - loss: 0.5340 - accuracy: 0.8195 - val_loss: 0.3208 - val_accuracy: 0.8911 - lr: 2.0000e-04
Test loss: 0.34227442741394043
Test accuracy: 0.882001519203186
